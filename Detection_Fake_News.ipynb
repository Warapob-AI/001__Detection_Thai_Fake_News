{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from pythainlp.corpus import thai_stopwords, thai_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pythainlp.tokenize import Tokenizer\n",
    "from pythainlp.util import dict_trie\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data(filename): \n",
    "    import codecs\n",
    "\n",
    "    with codecs.open(filename, 'r', 'utf-8') as file: \n",
    "        return [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatization(data_list): \n",
    "    word_lemmatization = {\n",
    "        r'รมว\\.กต\\.?': 'รัฐมนตรีว่าการกระทรวงการต่างประเทศของไทย',\n",
    "        r'รมว\\.กต': 'รัฐมนตรีว่าการกระทรวงการต่างประเทศของไทย',\n",
    "\n",
    "        r'กก\\.บห\\.?': 'คณะกรรมการบริหาร',\n",
    "        r'กกบห\\.': 'คณะกรรมการบริหาร',\n",
    "\n",
    "        r'ผบ\\.สส\\.?': 'ผู้บัญชาการทหารสูงสุด',\n",
    "        r'ผบสส\\.': 'ผู้บัญชาการทหารสูงสุด',\n",
    "\n",
    "        r'ส\\.ป\\.ส\\.ช\\.?': 'สำนักงานหลักประกันสุขภาพแห่งชาติ',\n",
    "        r'สปสช\\.': 'สำนักงานหลักประกันสุขภาพแห่งชาติ',\n",
    "\n",
    "        r'ก\\.ส\\.ท\\.ช\\.?': 'คณะกรรมการกิจการกระจายเสียงกิจการโทรทัศน์และกิจการโทรคมนาคมแห่งชาติ',\n",
    "        r'กสทช\\.': 'คณะกรรมการกิจการกระจายเสียงกิจการโทรทัศน์และกิจการโทรคมนาคมแห่งชาติ',\n",
    "\n",
    "        r'ร\\.ม\\.ว\\.?': 'รัฐมนตรีว่าการ',\n",
    "        r'รมว\\.': 'รัฐมนตรีว่าการ',\n",
    "\n",
    "        r'ส\\.ค\\.ร\\.?': 'สำนักงานคณะกรรมการนโยบายรัฐวิสาหกิจ',\n",
    "        r'สคร\\.': 'สำนักงานคณะกรรมการนโยบายรัฐวิสาหกิจ',\n",
    "\n",
    "        r'ค\\.ร\\.ม\\.?': 'คณะรัฐมนตรี',\n",
    "        r'ครม\\.': 'คณะรัฐมนตรี',\n",
    "\n",
    "        r'ส\\.ม\\.ช\\.?': 'สำนักงานสภาความมั่นคงแห่งชาติ',\n",
    "        r'สมช\\.': 'สำนักงานสภาความมั่นคงแห่งชาติ',\n",
    "\n",
    "        r'น\\.ร\\.ต\\.?': 'นักเรียนนายร้อยตำรวจ',\n",
    "        r'นรต\\.': 'นักเรียนนายร้อยตำรวจ',\n",
    "\n",
    "        r'ส\\.ส\\.ส\\.?': 'สำนักงานกองทุนสนับสนุนการสร้างเสริมสุขภาพ',\n",
    "        r'สสส\\.': 'สำนักงานกองทุนสนับสนุนการสร้างเสริมสุขภาพ',\n",
    "\n",
    "        r'ผ\\.บ\\.ช\\.': 'ผู้บัญชาการ',\n",
    "        r'ผบช\\.': 'ผู้บัญชาการ',\n",
    "\n",
    "        r'ก\\.อ\\.ช\\.?': 'กองทุนการออมแห่งชาติ',\n",
    "        r'กอช\\.': 'กองทุนการออมแห่งชาติ',\n",
    "\n",
    "        r'ส\\.ป\\.ป\\.?': 'สาธารณรัฐประชาธิปไตยประชาชน',\n",
    "        r'สปป\\.': 'สาธารณรัฐประชาธิปไตยประชาชน',\n",
    "\n",
    "        r'ก\\.ส\\.ร\\.?': 'กรมสวัสดิการและคุ้มครองแรงงาน',\n",
    "        r'กสร\\.': 'กรมสวัสดิการและคุ้มครองแรงงาน',\n",
    "\n",
    "        r'ก\\.ท\\.ม\\.?': 'กรุงเทพมหานคร',\n",
    "        r'กทม\\.': 'กรุงเทพมหานคร',\n",
    "\n",
    "        r'ก\\.ก\\.ต\\.?': 'คณะกรรมการการเลือกตั้ง',\n",
    "        r'กกต\\.': 'คณะกรรมการการเลือกตั้ง',\n",
    "\n",
    "        r'อ\\.บ\\.จ\\.?': 'องค์การบริหารส่วนจังหวัด',\n",
    "        r'อบจ\\.': 'องค์การบริหารส่วนจังหวัด',\n",
    "\n",
    "        r'พ\\.ร\\.บ\\.?': 'พระราชบัญญัติ',\n",
    "        r'พรบ\\.': 'พระราชบัญญัติ',\n",
    "\n",
    "        r'ผ\\.ก\\.ก\\.?': 'ผู้กำกับการ',\n",
    "        r'ผกก\\.': 'ผู้กำกับการ',\n",
    "\n",
    "        r'ก\\.ก\\.ร\\.?': 'คณะกรรมการร่วมภาคเอกชน3สถาบัน',\n",
    "        r'กกร\\.': 'คณะกรรมการร่วมภาคเอกชน3สถาบัน',\n",
    "\n",
    "        r'ส\\.พ\\.ฐ\\.?': 'สำนักงานคณะกรรมการการศึกษาขั้นพื้นฐาน',\n",
    "        r'สพฐ\\.': 'สำนักงานคณะกรรมการการศึกษาขั้นพื้นฐาน',\n",
    "\n",
    "        r'ม\\.ท\\.ร\\.?': 'มหาวิทยาลัยเทคโนโลยีราชมงคล',\n",
    "        r'มทร\\.': 'มหาวิทยาลัยเทคโนโลยีราชมงคล',\n",
    "\n",
    "        r'จ\\.น\\.ท\\.?': 'เจ้าหน้าที่',\n",
    "        r'จนท\\.': 'เจ้าหน้าที่',\n",
    "\n",
    "        r'ป\\.ป\\.ง\\.?': 'สำนักงานป้องกันและปราบปรามการฟอกเงิน',\n",
    "        r'ปปง\\.': 'สำนักงานป้องกันและปราบปรามการฟอกเงิน',\n",
    "\n",
    "        r'ธ\\.ก\\.ส\\.?': 'ธนาคารเพื่อการเกษตรและสหกรณ์การเกษตร',\n",
    "        r'ธกส\\.': 'ธนาคารเพื่อการเกษตรและสหกรณ์การเกษตร',\n",
    "\n",
    "        r'ส\\.ป\\.ส\\.?': 'สำนักงานประกันสังคม',\n",
    "        r'สปส\\.': 'สำนักงานประกันสังคม',\n",
    "\n",
    "        r'พ\\.น\\.ง\\.?': 'พนักงาน',\n",
    "        r'พนง\\.': 'พนักงาน',\n",
    "\n",
    "        r'ก\\.ส\\.ท\\.?': 'การสื่อสารแห่งประเทศไทย',\n",
    "        r'กสท\\.': 'การสื่อสารแห่งประเทศไทย',\n",
    "\n",
    "        r'ก\\.ฟ\\.ภ\\.?': 'การไฟฟ้าส่วนภูมิภาค',\n",
    "        r'กฟภ\\.': 'การไฟฟ้าส่วนภูมิภาค',\n",
    "\n",
    "        r'ร\\.ฟ\\.ม\\.?': 'การรถไฟฟ้าขนส่งมวลชนแห่งประเทศไทย',\n",
    "        r'รฟม\\.': 'การรถไฟฟ้าขนส่งมวลชนแห่งประเทศไทย',\n",
    "\n",
    "        r'ป\\.ช\\.ช\\.?': 'ประชาชน',\n",
    "        r'ปชช\\.': 'ประชาชน',\n",
    "\n",
    "        r'ต\\.ก\\.?': 'กองแผนงานกิจการพิเศษ',\n",
    "        r'ตก\\.': 'กองแผนงานกิจการพิเศษ',\n",
    "\n",
    "        r'พ\\.ม\\.?': 'กระทรวงการพัฒนาสังคมและความมั่นคงของมนุษย์',\n",
    "        r'พม\\.': 'กระทรวงการพัฒนาสังคมและความมั่นคงของมนุษย์',\n",
    "        \n",
    "        r'ก\\.ม\\.?': 'กฎหมาย',\n",
    "        r'กม\\.': 'กฎหมาย',\n",
    "\n",
    "        r'ม\\.ท\\.?': 'กระทรวงมหาดไทย',\n",
    "        r'มท\\.': 'กระทรวงมหาดไทย',\n",
    "\n",
    "        r'ต\\.ม\\.?': 'กองตรวจคนเข้าเมือง',\n",
    "        r'ตม\\.': 'กองตรวจคนเข้าเมือง',\n",
    "\n",
    "        r'ส\\.ธ\\.?': 'กระทรวงสาธารณสุข',\n",
    "        r'สธ\\.': 'กระทรวงสาธารณสุข',\n",
    "\n",
    "        r'อ\\.ย\\.?': 'สํานักงานคณะกรรมการอาหารและยา',\n",
    "        r'อย\\.': 'สํานักงานคณะกรรมการอาหารและยา',\n",
    "\n",
    "        r'ผ\\.อ\\.?': 'ผู้อำนวยการ',\n",
    "        r'ผอ\\.': 'ผู้อำนวยการ',\n",
    "\n",
    "        r'ผ\\.ก\\.?': 'กองแผนงานกิจการพิเศษ',\n",
    "        r'ผก\\.': 'กองแผนงานกิจการพิเศษ',\n",
    "\n",
    "        r'ร\\.พ\\.?': 'โรงพยาบาล',\n",
    "        r'รพ\\.': 'โรงพยาบาล',\n",
    "\n",
    "        r'ส\\.ภ\\.?': 'สถานีตำรวจภูธร',\n",
    "        r'สภ\\.': 'สถานีตำรวจภูธร',\n",
    "\n",
    "        r'น\\.ร\\.?': 'นักเรียน',\n",
    "        r'นร\\.': 'นักเรียน',\n",
    "        \n",
    "        r'ร\\.ร\\.?': 'โรงเรียน',\n",
    "        r'รร\\.': 'โรงเรียน',\n",
    "\n",
    "        r'ท\\.ส\\.?': 'กระทรวงทรัพยากรธรรมชาติและสิ่งแวดล้อม',\n",
    "        r'ทส\\.': 'กระทรวงทรัพยากรธรรมชาติและสิ่งแวดล้อม',\n",
    "\n",
    "        r'น\\.ช\\.?': 'นักโทษชาย',\n",
    "        r'นช\\.': 'นักโทษชาย',\n",
    "\n",
    "        r'ศ\\.ธ\\.?': 'กระทรวงศึกษาธิการ',\n",
    "        r'ศธ\\.': 'กระทรวงศึกษาธิการ',\n",
    "\n",
    "        r'ผ\\.บ\\.?': 'ผู้บัญชาการ',\n",
    "        r'ผบ\\.': 'ผู้บัญชาการ',\n",
    "\n",
    "        r'ล\\.บ\\.?': 'ล้านบาท',\n",
    "        r'ลบ\\.': 'ล้านบาท',\n",
    "\n",
    "        r'ม\\.ค\\.?': 'มกราคม', \n",
    "        r'มค\\.': 'มกราคม',\n",
    "\n",
    "        r'ก\\.พ\\.?': 'กุมภาพันธ์',\n",
    "        r'กพ\\.': 'กุมภาพันธ์',\n",
    "\n",
    "        r'มี\\.ค\\.?': 'มีนาคม',\n",
    "        r'มีค\\.': 'มีนาคม',\n",
    "\n",
    "        r'เม\\.ย\\.?': 'เมษายน',\n",
    "        r'เมย\\.': 'เมษายน',\n",
    "\n",
    "        r'พ\\.ค\\.?': 'พฤษภาคม',\n",
    "        r'พค\\.': 'พฤษภาคม',\n",
    "\n",
    "        r'มิ\\.ย\\.?': 'มิถุนายน',\n",
    "        r'มิย\\.': 'มิถุนายน',\n",
    "\n",
    "        r'ก\\.ค\\.?': 'กรกฎาคม',\n",
    "        r'กค\\.': 'กรกฎาคม',\n",
    "\n",
    "        r'ส\\.ค\\.?': 'สิงหาคม',\n",
    "        r'สค\\.': 'สิงหาคม',\n",
    "\n",
    "        r'ก\\.ย\\.?': 'กันยายน',\n",
    "        r'กย\\.': 'กันยายน',\n",
    "\n",
    "        r'ต\\.ค\\.?': 'ตุลาคม',\n",
    "        r'ตค\\.': 'ตุลาคม',\n",
    "\n",
    "        r'พ\\.ย\\.?': 'พฤศจิกายน',\n",
    "        r'พย\\.': 'พฤศจิกายน',\n",
    "\n",
    "        r'ธ\\.ค\\.?': 'ธันวาคม',\n",
    "        r'ธค\\.': 'ธันวาคม',\n",
    "\n",
    "        r'ม\\.33': 'มาตรา33',\n",
    "        r'ม\\.39': 'มาตรา39',\n",
    "        r'ม\\.40': 'มาตรา40',\n",
    "        r'ม\\.112': 'มาตรา112',\n",
    "    }\n",
    "\n",
    "    for pattern, full_month in word_lemmatization.items():\n",
    "        data_list = [re.sub(pattern, full_month, t) for t in data_list]\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Number(data_list): \n",
    "    text_no_numbers = [re.sub(r'\\d+', '', t) for t in data_list]\n",
    "    return text_no_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Special_character(data_list):\n",
    "    cleaned_data_list = [re.sub(r'[^a-zA-Z0-9ก-ฮะ-์ํา]+', '', t) for t in data_list]\n",
    "    return cleaned_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(data_list): \n",
    "    custom_words_list = set(thai_words())\n",
    "    custom_words_list.add('รัฐมนตรีว่าการ')\n",
    "    custom_words_list.add('สำนักงานกองทุนสนับสนุนการสร้างเสริมสุขภาพ')\n",
    "    custom_words_list.add('ตำรวจไซเบอร์')\n",
    "    custom_words_list.add('ไม้มิสวาก')\n",
    "    custom_words_list.add('ไม่มี')\n",
    "    custom_words_list.add('คอล')\n",
    "    custom_words_list.add('ผู้บัญชาการไซเบอร์')\n",
    "    custom_words_list.add('ของฟรีไม่มีในโลก')\n",
    "    custom_words_list.add('ล้านบาท')\n",
    "    custom_words_list.add('ทุบสถิติ')\n",
    "    custom_words_list.add('ไม่ให้')\n",
    "    custom_words_list.add('แสนราย')\n",
    "    custom_words_list.add('พันล้าน')\n",
    "    custom_words_list.add('กาซ่า')\n",
    "    custom_words_list.add('กองทุนการออมแห่งชาติ')\n",
    "    custom_words_list.add('ต้องการ')\n",
    "    custom_words_list.add('แรงงาน')\n",
    "    custom_words_list.add('จูราสสิคเวิลด์')\n",
    "    custom_words_list.add('ไม่ใช่')\n",
    "    custom_words_list.add('จู่โจม')\n",
    "    custom_words_list.add('ไม่ให้')\n",
    "    custom_words_list.add('กรมสวัสดิการและคุ้มครองแรงงาน')\n",
    "    custom_words_list.add('กรมควบคุมโรค')\n",
    "    custom_words_list.add('อนามัย')\n",
    "    custom_words_list.add('บุหรี่ไฟฟ้า')\n",
    "    custom_words_list.add('องค์กรพลังงาน')\n",
    "    custom_words_list.add('แล้ว')\n",
    "    custom_words_list.add('คริป')\n",
    "    custom_words_list.add('คริปโต')\n",
    "    custom_words_list.add('กระทรวงการพัฒนาสังคมและความมั่นคงของมนุษย์')\n",
    "    custom_words_list.add('อันตราย')\n",
    "    custom_words_list.add('องค์การค้า')\n",
    "    custom_words_list.add('คริสไรท์')\n",
    "    custom_words_list.add('ไม่น่า')\n",
    "    custom_words_list.add('รูบิโอ')\n",
    "    custom_words_list.add('เนทันยาฮู')\n",
    "    custom_words_list.add('สีจิ้นผิง')\n",
    "    custom_words_list.add('ทรีนิตี้')\n",
    "    custom_words_list.add('เฟนทานิล')\n",
    "    custom_words_list.add('อ่าว')\n",
    "    custom_words_list.add('กวนตานาโม')\n",
    "    custom_words_list.add('กล้าธรรม')\n",
    "    custom_words_list.add('เซเลนสกี')\n",
    "    custom_words_list.add('ดีอี')\n",
    "    custom_words_list.add('ออนแทรีโอ')\n",
    "    custom_words_list.add('สมศักดิ์')\n",
    "    custom_words_list.add('อิชิบะ')\n",
    "    custom_words_list.add('บิ๊กอ้วน')\n",
    "    custom_words_list.add('บูลลี่')\n",
    "    custom_words_list.add('ผู้บัญชาการทหารสูงสุด')\n",
    "    custom_words_list.add('ขึ้น')\n",
    "    custom_words_list.add('ราชกรมทัณฑ์')\n",
    "    custom_words_list.add('รายงาน')\n",
    "    custom_words_list.add('สํานักงานคณะกรรมการอาหารและยา')\n",
    "    custom_words_list.add('คณะกรรมการกิจการกระจายเสียงกิจการโทรทัศน์และกิจการโทรคมนาคมแห่งชาติ')\n",
    "    \n",
    "    trie = dict_trie(dict_source=custom_words_list)\n",
    "    _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')\n",
    "\n",
    "    list_data = []\n",
    "    for i in range(len(data_list)): \n",
    "        word = _tokenizer.word_tokenize((data_list[i]))\n",
    "        word = \" \".join(word)\n",
    "        list_data.append(word)\n",
    "    return list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stopwords(data_list): \n",
    "    cleaned_data_list = []\n",
    "    for data in data_list:\n",
    "        words = \" \".join(word for word in data.split() if word not in thai_stopwords())\n",
    "        cleaned_data_list.append(words)\n",
    "    return cleaned_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lowercase(data_list):\n",
    "    return [text.lower() for text in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data(list_fake_news, list_true_news):\n",
    "    list_fake_news = list_fake_news\n",
    "    list_true_news = list_true_news\n",
    "\n",
    "    sentences = list_fake_news + list_true_news\n",
    "    labels = [0] * len(list_fake_news) + [1] * len(list_true_news)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(sentences, labels, test_size = 0.1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = Lemmatization(text)\n",
    "    text = Remove_Special_character(text)\n",
    "    text = Tokenization(text)\n",
    "    text = Lowercase(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, batch_size=16):\n",
    "    model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    count = 0\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        batch = preprocess_text(batch)\n",
    "        # print(batch)\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        count += 1\n",
    "        # print(count)\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_To_BERT(x_train, x_test): \n",
    "    x_train_vectorizer = encode_sentences(x_train)\n",
    "    print('Test x_train_vectorizer Success ✅')\n",
    "    x_test_vectorizer = encode_sentences(x_test)\n",
    "    print('Test x_test_vectorizer Success ✅')\n",
    "\n",
    "    return x_train_vectorizer, x_test_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Vectorizer(x_train_vectorizer, x_test_vectorizer, y_train, y_test):\n",
    "    joblib.dump(x_train_vectorizer, 'x_train_wangchan.pkl')\n",
    "    joblib.dump(x_test_vectorizer, 'x_test_wangchan.pkl')\n",
    "    joblib.dump(y_train, 'y_train_wangchan.pkl')\n",
    "    joblib.dump(y_test, 'y_test_wangchan.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Vectorizer(x_train, x_test, y_train, y_test): \n",
    "    x_train_vectorizer = joblib.load(x_train)\n",
    "    x_test_vectorizer = joblib.load(x_test)\n",
    "    y_train = joblib.load(y_train)\n",
    "    y_test = joblib.load(y_test)\n",
    "\n",
    "    return x_train_vectorizer, x_test_vectorizer, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fake_news = Load_data('fake_news.txt')\n",
    "list_true_news = Load_data('true_news.txt')\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = Import_Data(list_fake_news, list_true_news)\n",
    "x_train_vectorizer, x_test_vectorizer = Text_To_BERT(x_train, x_test)\n",
    "Save_Vectorizer(x_train_vectorizer, x_test_vectorizer, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression(x_train, x_test, y_train, y_test):\n",
    "    name = 'Logistic_Regression'\n",
    "    model = LogisticRegression(max_iter=2000, C=1, tol=0.001)\n",
    "    model.fit(x_train, y_train) \n",
    "    y_pred = model.predict(x_test)  \n",
    "\n",
    "    return name, y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification_Report(name, y_test, y_pred): \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'ความแม่นยำของโมเดล {name} : {accuracy * 100:.2f}')\n",
    "\n",
    "    classification = classification_report(y_test, y_pred, target_names=['Fake News', 'True News'])\n",
    "    print(classification)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(f'True Positive (TP): {tp}')\n",
    "        print(f'False Positive (FP): {fp}')\n",
    "        print(f'True Negative (TN): {tn}')\n",
    "        print(f'False Negative (FN): {fn}')\n",
    "    else:\n",
    "        print(\"Confusion matrix is not binary. Please check the number of classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentence(data_list, model_train):\n",
    "    global count_true_news, count_fake_news\n",
    "    sentence_vectorizer = encode_sentences(data_list)\n",
    "    sentence_pred = model_train.predict(sentence_vectorizer)\n",
    "    sentence_probs = model_train.predict_proba(sentence_vectorizer)\n",
    "    \n",
    "    count_true_news = 0\n",
    "    count_fake_news = 0\n",
    "\n",
    "    for i in range(len(sentence_pred)):\n",
    "        sentence_accuracy = max(sentence_probs[i]) * 100  # ความมั่นใจของโมเดล\n",
    "\n",
    "        if sentence_pred[i] == 'ข่าวจริง' or sentence_pred[i] == 1:\n",
    "            # if (len(data_list) <= 1): \n",
    "            #     print(f'ข่าวที่ {i + 1}: {data_list[i]} || ข่าวจริง: {sentence_accuracy:.2f}%')\n",
    "            count_true_news += 1\n",
    "        else:\n",
    "            # if (len(data_list) <= 1): \n",
    "            #     print(f'ข่าวที่ {i + 1}: {data_list[i]} || ข่าวปลอม:{sentence_accuracy:.2f}%')\n",
    "            count_fake_news += 1\n",
    "\n",
    "    labels = ''\n",
    "    if (sentence_pred[i] == 0): \n",
    "        labels = 'ข่าวปลอม'\n",
    "    else: \n",
    "        labels = 'ข่าวจริง'\n",
    "\n",
    "    return f'ข่าว: {data_list[0]} || {labels}: {sentence_accuracy:.2f}%'\n",
    "\n",
    "def Test_Fake_News(data_list, model_train): \n",
    "    Sentence(data_list, model_train)\n",
    "    print(f'ในการทดสอบข่าวปลอมทั้งหมด {len(data_list)} ข่าว, โมเดลทำนายผิดว่าเป็นข่าวจริง : {count_true_news} ข่าว')\n",
    "\n",
    "def Test_True_News(data_list, model_train): \n",
    "    Sentence(data_list, model_train)\n",
    "    print(f'ในการทดสอบข่าวจริงทั้งหมด {len(data_list)} ข่าว, โมเดลทำนายผิดว่าเป็นข่าวปลอม : {count_fake_news} ข่าว')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = Load_Vectorizer('x_train_wangchan.pkl', 'x_test_wangchan.pkl', 'y_train_wangchan.pkl', 'y_test_wangchan.pkl')\n",
    "name, y_pred, model_train = Logistic_Regression(x_train, x_test, y_train, y_test)\n",
    "Classification_Report(name, y_test, y_pred)\n",
    "joblib.dump(model_train, 'model.lr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ข่าว: ‘อิ๊งค์’ ดูความคืบหน้าค้นหาผู้สูญหายตึก สตง.ถล่ม พันธมิตรแรงงานฯ คุกเข่าไหว้ทั้งน้ำตาวอนช่วยเหลือ || ข่าวจริง: 98.52%\n"
     ]
    }
   ],
   "source": [
    "sentence = str(input('\\nEnter Sentence : '))\n",
    "models = joblib.load('model.lr.pkl')\n",
    "result = Sentence([sentence], models)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
